"""
Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐÐÐ¯ LLM ÑÐµÑÑÐ¸Ñ Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¼ Ð½Ð°ÐºÐ¾Ð¿Ð»ÐµÐ½Ð¸ÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
"""

import re
import json
import logging
from typing import Dict, List, Optional, Tuple
from .openrouter_client import MessageContext
from .prompt_manager import PromptManager
from .config import load_config

logger = logging.getLogger(__name__)


class InterpretationStage:
    """Ð­Ñ‚Ð°Ð¿Ñ‹ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ð¸"""
    SYSTEM = "system"
    SPREAD_CONTEXT = "spread_context" 
    PSYCHOLOGICAL_ANALYSIS = "psychological_analysis"
    CONTEXT_ANALYSIS = "context_analysis"
    SYNTHESIS = "synthesis"
    FINAL_RESPONSE = "final_response"


class FixedLLMSession:
    """
    Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐÐÐ¯ LLM ÑÐµÑÑÐ¸Ñ Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð¹ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð¾Ð¹:
    - Ð’ÑÐµ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ‹ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑŽÑ‚ÑÑ Ð² ÐµÐ´Ð¸Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚
    - ÐžÐ”Ð˜Ð Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð² ÐºÐ¾Ð½Ñ†Ðµ
    - LLM Ð²Ð¸Ð´Ð¸Ñ‚ Ð²ÑÑŽ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð²ÑÐµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸
    """
    
    def __init__(self, prompt_manager: PromptManager, model_name: str = None):
        self.prompt_manager = prompt_manager
        self.config = load_config()
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ MessageContext Ñ mode=2 (ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð²Ð°Ð¶Ð½Ð¾!)
        self.context = MessageContext(task_prompt=None)
        
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸Ð»Ð¸ Ð±ÐµÑ€ÐµÐ¼ Ð¸Ð· ÐºÐ¾Ð½Ñ„Ð¸Ð³Ð°
        if model_name is None:
            model_name = self.config.get("model_name", "deepseek/deepseek-chat-v3-0324:free")
        
        # ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð´Ð»Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
        self.model_name = model_name
        self.api_key = self.config.get("openrouter_api_key")
        self.max_tokens = self.config.get("max_response_tokens", 8000)
        self.temperature = self.config.get("temperature", 0.3)
        
        # Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ ÑÐµÑÑÐ¸Ð¸
        self.current_stage = None
        self.user_data = {}
        self.spread_data = {}
        self.generated_questions = []
        
    def setup_spread(self, spread_type: str, selected_cards: List[Dict], 
                    spread_config: Dict, user_name: str, user_age: int, 
                    magic_number: int):
        """ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ€Ð°ÑÐºÐ»Ð°Ð´Ð°"""
        self.spread_data = {
            'spread_type': spread_type,
            'selected_cards': selected_cards,
            'spread_config': spread_config,
            'user_name': user_name,
            'user_age': user_age,
            'magic_number': magic_number
        }
        
        self.user_data = {
            'name': user_name,
            'age': user_age,
            'magic_number': magic_number
        }
    
    def add_spread_context(self, spread_type: str):
        """Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ñ€Ð°ÑÐºÐ»Ð°Ð´Ð° Ð² ÑÐµÑÑÐ¸ÑŽ"""
        context_prompt = self.prompt_manager.get_spread_context_prompt(spread_type)
        self.context.add_user_message(context_prompt)
        self.current_stage = InterpretationStage.SPREAD_CONTEXT
        logger.info(f"Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ñ€Ð°ÑÐºÐ»Ð°Ð´Ð°: {spread_type}")
    
    def build_complete_context(self, preliminary_answers: List[str], 
                              additional_answers: List[str] = None) -> bool:
        """
        ÐšÐ›Ð®Ð§Ð•Ð’ÐžÐ™ ÐœÐ•Ð¢ÐžÐ”: Ð¡Ñ‚Ñ€Ð¾Ð¸Ñ‚ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ ÑÐ¾ Ð’Ð¡Ð•ÐœÐ˜ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°Ð¼Ð¸
        Ð­Ñ‚Ð¾ Ð¸ ÐµÑÑ‚ÑŒ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð°Ñ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð¾ LLM_algorithm.md!
        """
        try:
            # 1. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ð°Ñ Ð¿ÐµÑ€ÑÐ¾Ð½Ð° (01_system_persona.md)
            system_prompt = self.prompt_manager.get_system_persona(
                name=self.user_data['name'],
                age=self.user_data['age']
            )
            self.context = MessageContext(task_prompt=system_prompt)
            logger.info("1ï¸âƒ£ Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚")
            
            # 2. ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ñ€Ð°ÑÐºÐ»Ð°Ð´Ð° (02_*_context.md)
            spread_type = self.spread_data.get('spread_type', '')
            spread_prompt = self.prompt_manager.get_spread_context(
                spread_type=spread_type,
                selected_cards=self.spread_data['selected_cards'],
                positions=self.spread_data['spread_config'].get('card_meanings', [])
            )
            self.context.add_user_message(spread_prompt)
            logger.info(f"2ï¸âƒ£ Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ñ€Ð°ÑÐºÐ»Ð°Ð´Ð°: {spread_type}")
            
            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹
            if preliminary_answers:
                answers_text = "\\n".join([f"{i+1}. {answer}" for i, answer in enumerate(preliminary_answers)])
                self.context.add_user_message(f"ÐŸÐ Ð•Ð”Ð’ÐÐ Ð˜Ð¢Ð•Ð›Ð¬ÐÐ«Ð• ÐžÐ¢Ð’Ð•Ð¢Ð« ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐ¢Ð•Ð›Ð¯:\\n{answers_text}")
                logger.info("ðŸ“ Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹")
            
            # 3. ÐŸÑÐ¸Ñ…Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ (03_psychological_analysis_questions.md) 
            analysis_prompt = self.prompt_manager.get_psychological_analysis_prompt(
                user_answers=preliminary_answers or []
            )
            self.context.add_user_message(analysis_prompt)
            logger.info("3ï¸âƒ£ Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð¿ÑÐ¸Ñ…Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°")
            
            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ
            if additional_answers:
                additional_text = "\\n".join([f"{i+1}. {answer}" for i, answer in enumerate(additional_answers)])
                self.context.add_user_message(f"Ð”ÐžÐŸÐžÐ›ÐÐ˜Ð¢Ð•Ð›Ð¬ÐÐ«Ð• ÐžÐ¢Ð’Ð•Ð¢Ð« ÐÐ Ð£Ð¢ÐžÐ§ÐÐ¯Ð®Ð©Ð˜Ð• Ð’ÐžÐŸÐ ÐžÐ¡Ð«:\\n{additional_text}")
                logger.info("ðŸ“ Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹")
            
            # 4. ÐÐ½Ð°Ð»Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð¸ ÐºÐ°Ñ€Ñ‚ (04_context_analysis_and_card_interpretation.md)
            context_analysis_prompt = self.prompt_manager.get_context_analysis_prompt()
            self.context.add_user_message(context_analysis_prompt)
            logger.info("4ï¸âƒ£ Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°")
            
            # 5. Ð“Ð»ÑƒÐ±Ð¾ÐºÐ¸Ð¹ ÑÐ¸Ð½Ñ‚ÐµÐ· (05_deep_synthesis_and_story_planning.md)
            synthesis_prompt = self.prompt_manager.get_synthesis_prompt()
            self.context.add_user_message(synthesis_prompt)
            logger.info("5ï¸âƒ£ Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð°")
            
            # 6. Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ (06_final_user_response.md)
            final_prompt = self.prompt_manager.get_final_response_prompt()
            self.context.add_user_message(final_prompt)
            logger.info("6ï¸âƒ£ Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð°")
            
            return True
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°: {e}")
            return False
    
    async def generate_complete_interpretation(self) -> Optional[str]:
        """
        Ð•Ð”Ð˜ÐÐ¡Ð¢Ð’Ð•ÐÐÐ«Ð™ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ðº LLM Ñ Ð¿Ð¾Ð»Ð½Ñ‹Ð¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼!
        Ð­Ñ‚Ð¾ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð°Ñ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ - LLM Ð²Ð¸Ð´Ð¸Ñ‚ Ð’Ð¡Ð• Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ‹ Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ Ðº ÐºÑ€Ð°Ñ‚ÐºÐ¾ÑÑ‚Ð¸!
        """
        try:
            messages = self.context.get_message_history()
            
            # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸ 
            logger.info(f"ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ñ {len(messages)} ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ")
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‡Ñ‚Ð¾ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ðµ Ðº ÐºÑ€Ð°Ñ‚ÐºÐ¾ÑÑ‚Ð¸ Ð¿Ñ€Ð¸ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚
            full_context = "\\n".join([msg['content'][0]['text'] for msg in messages])
            has_brevity = "Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ðµ Ðº ÐºÑ€Ð°Ñ‚ÐºÐ¾ÑÑ‚Ð¸" in full_context
            logger.info(f"Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ðµ Ðº ÐºÑ€Ð°Ñ‚ÐºÐ¾ÑÑ‚Ð¸ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ: {has_brevity}")
            
            from .openrouter_client import send_request
            response = await send_request(
                messages=messages,
                model=self.model_name,
                api_key=self.api_key,
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            
            self.current_stage = InterpretationStage.FINAL_RESPONSE
            logger.info("ðŸŽ¯ ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ñ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð° Ð·Ð° ÐžÐ”Ð˜Ð Ð·Ð°Ð¿Ñ€Ð¾Ñ!")
            
            # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸ÑŽ
            cleaned_response = self._extract_final_interpretation(response)
            return cleaned_response
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ð¸: {e}")
            return None
    
    def _extract_final_interpretation(self, response: str) -> str:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸ÑŽ Ð¸Ð· Ð¾Ñ‚Ð²ÐµÑ‚Ð° LLM"""
        try:
            # Ð¡Ð¿Ð¾ÑÐ¾Ð± 1: Ð˜Ñ‰ÐµÐ¼ Ð¼Ð°Ñ€ÐºÐµÑ€Ñ‹ [INTERPRETATION_START] Ð¸ [INTERPRETATION_END]
            interpretation_match = re.search(r'\\[INTERPRETATION_START\\](.*?)\\[INTERPRETATION_END\\]', 
                                           response, re.DOTALL | re.IGNORECASE)
            if interpretation_match:
                cleaned = interpretation_match.group(1).strip()
                logger.info("âœ… Ð˜Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð° Ð¿Ð¾ Ð¼Ð°Ñ€ÐºÐµÑ€Ð°Ð¼")
                return cleaned
            
            # Ð¡Ð¿Ð¾ÑÐ¾Ð± 2: Ð˜Ñ‰ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ Ð±Ð»Ð¾Ðº Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°
            lines = response.split('\\n')
            interpretation_lines = []
            capturing = False
            
            for line in lines:
                if 'INTERPRETATION_START' in line or capturing:
                    capturing = True
                    if 'INTERPRETATION_END' in line:
                        break
                    interpretation_lines.append(line)
            
            if interpretation_lines:
                result = '\\n'.join(interpretation_lines).strip()
                # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð¾Ñ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ñ… Ð¼Ð°Ñ€ÐºÐµÑ€Ð¾Ð²
                result = re.sub(r'\\[INTERPRETATION_(START|END)\\]', '', result).strip()
                logger.info("âœ… Ð˜Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð° Ð¿Ð¾ Ð±Ð»Ð¾ÐºÐ°Ð¼")
                return result
            
            # Ð¡Ð¿Ð¾ÑÐ¾Ð± 3: Ð‘ÐµÑ€ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 1000-2000 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² ÐºÐ°Ðº Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸ÑŽ
            if len(response) > 1000:
                result = response[-2000:].strip()
                logger.info("âš ï¸ Ð˜Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ñ Ð²Ð·ÑÑ‚Ð° ÐºÐ°Ðº Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ Ð±Ð»Ð¾Ðº")
                return result
            
            # Ð¡Ð¿Ð¾ÑÐ¾Ð± 4: Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð²ÐµÑÑŒ Ð¾Ñ‚Ð²ÐµÑ‚
            logger.warning("âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð²Ñ‹Ð´ÐµÐ»Ð¸Ñ‚ÑŒ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸ÑŽ, Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚")
            return response.strip()
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ð¸: {e}")
            return response.strip()
    
    def get_context_debug_info(self) -> Dict:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¾Ñ‚Ð»Ð°Ð´Ð¾Ñ‡Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ"""
        messages = self.context.get_message_history()
        full_context = "\\n".join([msg['content'][0]['text'] for msg in messages])
        
        return {
            'message_count': len(messages),
            'total_length': len(full_context),
            'has_brevity_instruction': 'Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ðµ Ðº ÐºÑ€Ð°Ñ‚ÐºÐ¾ÑÑ‚Ð¸' in full_context,
            'has_system_persona': '01_system_persona' in full_context,
            'has_spread_context': '02_' in full_context and '_context' in full_context,
            'context_preview': full_context[:500] + '...' if len(full_context) > 500 else full_context
        }